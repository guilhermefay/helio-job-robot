#!/usr/bin/env python3
"""
API HELIO - Streaming de Coleta de Vagas com LOGS DETALHADOS
Integra√ß√£o completa com sistema de agentes
"""
import os
import sys
import json
import time
import asyncio
import logging
from datetime import datetime
from typing import Dict, Any
from flask import Flask, request, Response, jsonify
from flask_cors import CORS, cross_origin

# Configurar logging detalhado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Adicionar o diret√≥rio atual ao path
sys.path.insert(0, os.path.dirname(__file__))

# Imports do sistema HELIO
try:
    from core.services.job_scraper import JobScraper
    from core.services.agente_1_palavras_chave import MPCCarolinaMartins
    from core.services.linkedin_apify_scraper import LinkedInApifyScraper
    logger.info("‚úÖ Imports do sistema HELIO carregados com sucesso")
except ImportError as e:
    logger.error(f"‚ùå Erro ao importar m√≥dulos HELIO: {e}")
    # Fallback para demonstra√ß√£o
    JobScraper = None
    MPCCarolinaMartins = None
    LinkedInApifyScraper = None

app = Flask(__name__)
CORS(app, origins=['https://agenteslinkedin.vercel.app', 'http://localhost:3000'])

@app.route('/api/health', methods=['GET'])
@cross_origin()
def health():
    """Health check com informa√ß√µes detalhadas do sistema"""
    
    # Verificar componentes do sistema
    sistema_status = {
        'status': 'ok',
        'service': 'helio-streaming-api',
        'timestamp': datetime.now().isoformat(),
        'componentes': {
            'job_scraper': JobScraper is not None,
            'mpc_agent': MPCCarolinaMartins is not None,
            'linkedin_scraper': LinkedInApifyScraper is not None,
            'apify_token': bool(os.getenv('APIFY_API_TOKEN')),
        },
        'versao': '2.0-streaming',
        'modo': 'production' if not app.debug else 'development'
    }
    
    # Log do status
    logger.info(f"üè• Health check - Sistema: {sistema_status}")
    
    return jsonify(sistema_status)

@app.route('/api/agent1/collect-jobs-stream', methods=['POST', 'OPTIONS'])
@cross_origin()
def collect_jobs_stream():
    """
    üöÄ ENDPOINT DE STREAMING PARA COLETA DE VAGAS
    Integra√ß√£o completa com sistema HELIO
    """
    
    if request.method == 'OPTIONS':
        return '', 200
    
    def generate_stream():
        """Gerador de stream com logs detalhados em tempo real"""
        
        try:
            # ================================
            # üìã VALIDA√á√ÉO DE ENTRADA
            # ================================
            
            logger.info("üöÄ INICIANDO STREAMING DE COLETA DE VAGAS")
            
            data = request.get_json()
            
            if not data:
                error_msg = "‚ùå Dados n√£o fornecidos na requisi√ß√£o"
                logger.error(error_msg)
                yield f"data: {json.dumps({'error': error_msg, 'timestamp': datetime.now().isoformat()})}\n\n"
                return
            
            # Extrair par√¢metros
            cargo = data.get('cargo_objetivo', 'Desenvolvedor')
            area = data.get('area_interesse', 'Tecnologia')
            localizacao = data.get('localizacao', 'Brasil')
            quantidade = data.get('total_vagas_desejadas', 800)
            
            logger.info(f"üìã Par√¢metros recebidos:")
            logger.info(f"   ‚Ä¢ Cargo: {cargo}")
            logger.info(f"   ‚Ä¢ √Årea: {area}")
            logger.info(f"   ‚Ä¢ Localiza√ß√£o: {localizacao}")
            logger.info(f"   ‚Ä¢ Quantidade: {quantidade}")
            
            # Enviar confirma√ß√£o inicial
            yield f"data: {json.dumps({
                'status': 'iniciando',
                'message': f'üöÄ Iniciando coleta de vagas para {cargo}...',
                'parametros': {
                    'cargo': cargo,
                    'area': area,
                    'localizacao': localizacao,
                    'quantidade': quantidade
                },
                'timestamp': datetime.now().isoformat()
            })}\n\n"
            
            # ================================
            # üîß VERIFICAR CONFIGURA√á√ÉO
            # ================================
            
            yield f"data: {json.dumps({
                'status': 'verificando_config',
                'message': 'üîß Verificando configura√ß√£o do sistema...',
                'timestamp': datetime.now().isoformat()
            })}\n\n"
            
            # Verificar token Apify
            apify_token = os.getenv('APIFY_API_TOKEN')
            if not apify_token:
                error_msg = "‚ùå APIFY_API_TOKEN n√£o configurado"
                logger.error(error_msg)
                yield f"data: {json.dumps({
                    'error': error_msg,
                    'detalhes': 'Configure a vari√°vel de ambiente APIFY_API_TOKEN',
                    'timestamp': datetime.now().isoformat()
                })}\n\n"
                return
            
            logger.info(f"‚úÖ APIFY_API_TOKEN configurado: {apify_token[:10]}...")
            
            yield f"data: {json.dumps({
                'status': 'config_ok',
                'message': '‚úÖ Configura√ß√£o verificada com sucesso',
                'timestamp': datetime.now().isoformat()
            })}\n\n"
            
            # ================================
            # üîÑ INICIALIZAR SCRAPERS
            # ================================
            
            yield f"data: {json.dumps({
                'status': 'inicializando_scrapers',
                'message': 'üîÑ Inicializando scrapers...',
                'timestamp': datetime.now().isoformat()
            })}\n\n"
            
            if JobScraper and LinkedInApifyScraper:
                logger.info("üîÑ Inicializando JobScraper...")
                job_scraper = JobScraper()
                
                logger.info("üîÑ Inicializando LinkedInApifyScraper...")
                linkedin_scraper = LinkedInApifyScraper()
                
                # Verificar credenciais
                if not linkedin_scraper.verificar_credenciais():
                    error_msg = "‚ùå Credenciais do LinkedIn Apify inv√°lidas"
                    logger.error(error_msg)
                    yield f"data: {json.dumps({
                        'error': error_msg,
                        'timestamp': datetime.now().isoformat()
                    })}\n\n"
                    return
                
                logger.info("‚úÖ Scrapers inicializados com sucesso")
                yield f"data: {json.dumps({
                    'status': 'scrapers_ok',
                    'message': '‚úÖ Scrapers inicializados com sucesso',
                    'timestamp': datetime.now().isoformat()
                })}\n\n"
                
                # ================================
                # üöÄ INICIAR COLETA STREAMING
                # ================================
                
                yield f"data: {json.dumps({
                    'status': 'iniciando_coleta',
                    'message': 'üöÄ Iniciando coleta via Apify...',
                    'timestamp': datetime.now().isoformat()
                })}\n\n"
                
                try:
                    # Usar m√©todo de streaming do JobScraper
                    run_id, dataset_id = job_scraper.iniciar_coleta_streaming(
                        area_interesse=area,
                        cargo_objetivo=cargo,
                        localizacao=localizacao,
                        total_vagas_desejadas=quantidade
                    )
                    
                    logger.info(f"‚úÖ Coleta iniciada - Run ID: {run_id}, Dataset ID: {dataset_id}")
                    
                    yield f"data: {json.dumps({
                        'status': 'coleta_iniciada',
                        'message': '‚úÖ Coleta iniciada no Apify',
                        'run_id': run_id,
                        'dataset_id': dataset_id,
                        'timestamp': datetime.now().isoformat()
                    })}\n\n"
                    
                    # ================================
                    # üîÑ POLLING COM ATUALIZA√á√ïES
                    # ================================
                    
                    vagas_coletadas = []
                    offset_atual = 0
                    tempo_inicio = time.time()
                    timeout_segundos = 420  # 7 minutos
                    
                    while True:
                        tempo_decorrido = time.time() - tempo_inicio
                        
                        # Verificar timeout
                        if tempo_decorrido > timeout_segundos:
                            logger.warning(f"‚è∞ Timeout ap√≥s {timeout_segundos} segundos")
                            yield f"data: {json.dumps({
                                'status': 'timeout',
                                'message': f'‚è∞ Timeout ap√≥s {timeout_segundos//60} minutos - finalizando com dados parciais',
                                'tempo_decorrido': int(tempo_decorrido),
                                'timestamp': datetime.now().isoformat()
                            })}\n\n"
                            break
                        
                        # Verificar status da execu√ß√£o
                        status_run = job_scraper.verificar_status_run(run_id)
                        logger.info(f"üìä Status da execu√ß√£o: {status_run}")
                        
                        yield f"data: {json.dumps({
                            'status': 'monitorando',
                            'run_status': status_run,
                            'message': f'üîç Monitorando execu√ß√£o... Status: {status_run}',
                            'tempo_decorrido': int(tempo_decorrido),
                            'timestamp': datetime.now().isoformat()
                        })}\n\n"
                        
                        # Contar resultados atuais
                        total_resultados = job_scraper.contar_resultados_dataset(dataset_id)
                        logger.info(f"üìä Total de resultados no dataset: {total_resultados}")
                        
                        # Se h√° novos resultados, buscar e enviar
                        if total_resultados > offset_atual:
                            novos_resultados = total_resultados - offset_atual
                            logger.info(f"üì• Buscando {novos_resultados} novos resultados...")
                            
                            # Buscar novos resultados
                            resultados_parciais = job_scraper.obter_resultados_parciais(
                                dataset_id, offset_atual, novos_resultados
                            )
                            
                            if resultados_parciais:
                                vagas_coletadas.extend(resultados_parciais)
                                offset_atual = total_resultados
                                
                                logger.info(f"‚úÖ {len(resultados_parciais)} novas vagas adicionadas")
                                
                                # Enviar atualiza√ß√£o com novas vagas
                                yield f"data: {json.dumps({
                                    'type': 'novas_vagas',
                                    'novas_vagas': resultados_parciais,
                                    'total_atual': len(vagas_coletadas),
                                    'meta': quantidade,
                                    'progresso': min((len(vagas_coletadas) / quantidade) * 100, 100),
                                    'message': f'üì• {len(resultados_parciais)} novas vagas coletadas (Total: {len(vagas_coletadas)})',
                                    'timestamp': datetime.now().isoformat()
                                })}\n\n"
                        
                        # Verificar se execu√ß√£o foi conclu√≠da
                        if status_run in ['SUCCEEDED', 'FAILED', 'ABORTED', 'TIMED-OUT']:
                            logger.info(f"üèÅ Execu√ß√£o finalizada com status: {status_run}")
                            break
                        
                        # Aguardar antes da pr√≥xima verifica√ß√£o
                        time.sleep(10)
                    
                    # ================================
                    # üèÅ FINALIZA√á√ÉO
                    # ================================
                    
                    # Buscar resultados finais se necess√°rio
                    total_final = job_scraper.contar_resultados_dataset(dataset_id)
                    if total_final > len(vagas_coletadas):
                        logger.info(f"üì• Buscando resultados finais...")
                        resultados_finais = job_scraper.obter_todos_resultados(dataset_id)
                        vagas_coletadas = resultados_finais
                    
                    logger.info(f"üèÅ Coleta finalizada - {len(vagas_coletadas)} vagas coletadas")
                    
                    yield f"data: {json.dumps({
                        'status': 'concluido',
                        'total_vagas': len(vagas_coletadas),
                        'meta': quantidade,
                        'percentual_atingido': (len(vagas_coletadas) / quantidade) * 100,
                        'tempo_total': int(time.time() - tempo_inicio),
                        'message': f'‚úÖ Coleta conclu√≠da! {len(vagas_coletadas)} vagas encontradas',
                        'timestamp': datetime.now().isoformat()
                    })}\n\n"
                    
                    # Enviar dados finais
                    yield f"data: {json.dumps({
                        'status': 'finalizado',
                        'total_vagas': len(vagas_coletadas),
                        'vagas': vagas_coletadas,
                        'timestamp': datetime.now().isoformat()
                    })}\n\n"
                    
                except Exception as e:
                    error_msg = f"‚ùå Erro durante coleta: {str(e)}"
                    logger.error(error_msg, exc_info=True)
                    yield f"data: {json.dumps({
                        'error': error_msg,
                        'detalhes': str(e),
                        'timestamp': datetime.now().isoformat()
                    })}\n\n"
            
            else:
                # ================================
                # üé≠ MODO DEMONSTRA√á√ÉO
                # ================================
                
                logger.warning("‚ö†Ô∏è Componentes do sistema n√£o dispon√≠veis - usando modo demonstra√ß√£o")
                
                yield f"data: {json.dumps({
                    'status': 'modo_demo',
                    'message': 'üé≠ Sistema em modo demonstra√ß√£o',
                    'timestamp': datetime.now().isoformat()
                })}\n\n"
                
                # Simular coleta progressiva
                vagas_demo = []
                for i in range(1, min(quantidade + 1, 51)):  # M√°ximo 50 para demo
                    vaga = {
                        'id': i,
                        'titulo': f'{cargo} - Posi√ß√£o {i}',
                        'empresa': f'Empresa Tech {i}',
                        'localizacao': localizacao,
                        'descricao': f'Vaga para {cargo} com experi√™ncia em tecnologias modernas.',
                        'link': f'https://linkedin.com/jobs/view/{1000000 + i}',
                        'nivel': 'Pleno' if i % 3 == 0 else 'J√∫nior',
                        'tipo': 'H√≠brido' if i % 2 == 0 else 'Remoto'
                    }
                    vagas_demo.append(vaga)
                    
                    # Enviar vaga individual
                    yield f"data: {json.dumps({
                        'type': 'nova_vaga',
                        'vaga': vaga,
                        'total_atual': i,
                        'meta': quantidade,
                        'progresso': (i / quantidade) * 100,
                        'timestamp': datetime.now().isoformat()
                    })}\n\n"
                    
                    # Update de progresso a cada 5 vagas
                    if i % 5 == 0:
                        yield f"data: {json.dumps({
                            'status': 'executando',
                            'total_vagas': i,
                            'message': f'‚è≥ Coletando... {i} vagas encontradas (DEMO)',
                            'timestamp': datetime.now().isoformat()
                        })}\n\n"
                    
                    time.sleep(0.5)  # Simular tempo real
                
                # Finalizar demo
                yield f"data: {json.dumps({
                    'status': 'concluido',
                    'total_vagas': len(vagas_demo),
                    'message': f'‚úÖ Demo conclu√≠da! {len(vagas_demo)} vagas simuladas',
                    'timestamp': datetime.now().isoformat()
                })}\n\n"
                
                yield f"data: {json.dumps({
                    'status': 'finalizado',
                    'total_vagas': len(vagas_demo),
                    'vagas': vagas_demo,
                    'timestamp': datetime.now().isoformat()
                })}\n\n"
            
        except Exception as e:
            error_msg = f"‚ùå Erro cr√≠tico no streaming: {str(e)}"
            logger.error(error_msg, exc_info=True)
            yield f"data: {json.dumps({
                'error': error_msg,
                'timestamp': datetime.now().isoformat()
            })}\n\n"
    
    return Response(
        generate_stream(),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Content-Type',
        }
    )

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    logger.info(f"üöÄ Iniciando HELIO Streaming API na porta {port}")
    app.run(host='0.0.0.0', port=port, debug=False) 