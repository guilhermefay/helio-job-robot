"""
Extrator de Palavras-Chave via IA - Sistema HELIO
Abordagem "Pura IA" seguindo metodologia Carolina Martins
Usa LLMs com grande janela de contexto para anÃ¡lise completa
"""

import os
import json
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import openai
from anthropic import Anthropic
import google.generativeai as genai
from collections import Counter
from dotenv import load_dotenv

# Garantir que as variÃ¡veis de ambiente sejam carregadas
load_dotenv()

class AIKeywordExtractor:
    """
    Extrator que envia todas as descriÃ§Ãµes de vagas para um LLM
    para anÃ¡lise completa e extraÃ§Ã£o inteligente de palavras-chave
    """
    
    def __init__(self):
        # Configurar clientes de IA
        self.anthropic_client = None
        self.openai_client = None
        self.gemini_model = None
        
        # Claude (200k tokens de contexto)
        if os.getenv('ANTHROPIC_API_KEY'):
            self.anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
        
        # OpenAI GPT-4 Turbo (128k tokens)
        if os.getenv('OPENAI_API_KEY'):
            openai.api_key = os.getenv('OPENAI_API_KEY')
            self.openai_client = openai
        
        # Google Gemini Pro (30k tokens input)
        if os.getenv('GOOGLE_API_KEY'):
            genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
            # Usando versÃ£o stable do Gemini Pro
            self.gemini_model = genai.GenerativeModel('gemini-pro')
    
    async def extrair_palavras_chave_ia(
        self, 
        vagas: List[Dict[str, Any]], 
        cargo_objetivo: str,
        area_interesse: str,
        callback_progresso: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        Extrai palavras-chave usando anÃ¡lise completa via IA
        
        Args:
            vagas: Lista de dicionÃ¡rios com descriÃ§Ãµes de vagas
            cargo_objetivo: Cargo alvo do usuÃ¡rio
            area_interesse: Ãrea de interesse
            callback_progresso: FunÃ§Ã£o para reportar progresso
            
        Returns:
            Dict com anÃ¡lise completa incluindo top 10 e categorizaÃ§Ã£o
        """
        
        if callback_progresso:
            await callback_progresso("Preparando descriÃ§Ãµes para anÃ¡lise IA...")
        
        # Preparar texto agregado
        texto_agregado = self._preparar_texto_vagas(vagas)
        total_vagas = len(vagas)
        
        print(f"\nðŸ¤– === EXTRAÃ‡ÃƒO VIA IA ===")
        print(f"ðŸ“Š Total de vagas: {total_vagas}")
        print(f"ðŸ“ Tamanho do texto: {len(texto_agregado)} caracteres")
        print(f"ðŸŽ¯ Cargo: {cargo_objetivo}")
        print(f"ðŸ¢ Ãrea: {area_interesse}")
        
        # Criar prompt sofisticado
        prompt = self._criar_prompt_extracao(
            texto_agregado, 
            cargo_objetivo, 
            area_interesse, 
            total_vagas
        )
        
        if callback_progresso:
            await callback_progresso("Analisando vagas com IA (isso pode levar 30-60 segundos)...")
        
        # Escolher modelo baseado em disponibilidade e tamanho
        resultado = None
        modelo_usado = None
        
        try:
            print(f"\nðŸ” Verificando modelos disponÃ­veis:")
            print(f"   - Gemini configurado: {self.gemini_model is not None}")
            print(f"   - Claude configurado: {self.anthropic_client is not None}")
            print(f"   - GPT-4 configurado: {self.openai_client is not None}")
            print(f"   - Tamanho do texto: {len(texto_agregado)} caracteres")
            
            # PreferÃªncia: Gemini Pro (30k tokens) > Claude (200k) > GPT-4 (128k)
            if self.gemini_model and len(texto_agregado) < 30000:
                try:
                    if callback_progresso:
                        await callback_progresso("Usando Google Gemini 2.5 Pro...")
                    print(f"âœ… Chamando Gemini 2.5 Pro...")
                    resultado = self._chamar_gemini(prompt)
                    modelo_usado = "gemini-2.5-pro"
                except Exception as gemini_error:
                    print(f"âš ï¸ Gemini falhou: {gemini_error}")
                    print(f"ðŸ”„ Tentando com Claude como fallback...")
                    
                    # Fallback para Claude se Gemini falhar
                    if self.anthropic_client and len(texto_agregado) < 180000:
                        if callback_progresso:
                            await callback_progresso("Usando Claude 3 Sonnet (fallback)...")
                        resultado = self._chamar_claude(prompt)
                        modelo_usado = "claude-3-sonnet"
                    else:
                        raise gemini_error
                
            elif self.anthropic_client and len(texto_agregado) < 180000:
                if callback_progresso:
                    await callback_progresso("Usando Claude 3 Sonnet...")
                print(f"âœ… Chamando Claude 3 Sonnet...")
                resultado = self._chamar_claude(prompt)
                modelo_usado = "claude-3-sonnet"
                
            elif self.openai_client and len(texto_agregado) < 100000:
                if callback_progresso:
                    await callback_progresso("Usando GPT-4 Turbo...")
                print(f"âœ… Chamando GPT-4 Turbo...")
                resultado = self._chamar_gpt4(prompt)
                modelo_usado = "gpt-4-turbo"
                
            else:
                raise Exception("Texto muito grande ou nenhuma API configurada. Por favor, configure pelo menos uma API key (GOOGLE_API_KEY, ANTHROPIC_API_KEY ou OPENAI_API_KEY)")
                
        except Exception as e:
            print(f"âŒ Erro na anÃ¡lise IA: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            if callback_progresso:
                await callback_progresso(f"Erro: {str(e)}")
            
            # NÃƒO usar fallback - Ã© melhor falhar do que dar resultado ruim
            erro_msg = f"NÃ£o foi possÃ­vel analisar as vagas com IA. {str(e)}"
            raise Exception(erro_msg)
        
        if callback_progresso:
            await callback_progresso("Processando resultados da IA...")
        
        # Processar e validar resultado
        resultado_final = self._processar_resultado_ia(resultado, modelo_usado)
        
        print(f"\nâœ… AnÃ¡lise concluÃ­da com {modelo_usado}")
        print(f"ðŸ” Top 10 palavras: {len(resultado_final.get('top_10_carolina_martins', []))}")
        print(f"ðŸ“Š Total palavras categorizadas: {resultado_final.get('total_palavras_unicas', 0)}")
        
        return resultado_final
    
    def _preparar_texto_vagas(self, vagas: List[Dict[str, Any]]) -> str:
        """Prepara texto agregado das vagas com separadores claros"""
        textos = []
        
        for i, vaga in enumerate(vagas, 1):
            titulo = vaga.get('titulo', 'Sem tÃ­tulo')
            empresa = vaga.get('empresa', 'Empresa nÃ£o informada')
            descricao = vaga.get('descricao', '')
            
            # Formato estruturado para melhor compreensÃ£o da IA
            texto_vaga = f"""
--- VAGA {i} ---
TÃ­tulo: {titulo}
Empresa: {empresa}
DescriÃ§Ã£o:
{descricao}
--- FIM VAGA {i} ---
"""
            textos.append(texto_vaga)
        
        return '\n'.join(textos)
    
    def _criar_prompt_extracao(
        self, 
        texto_vagas: str, 
        cargo_objetivo: str,
        area_interesse: str,
        total_vagas: int
    ) -> str:
        """Cria prompt sofisticado para extraÃ§Ã£o via IA"""
        
        prompt = f"""VocÃª Ã© o Agente de Palavras-chave do mÃ©todo Carreira MeteÃ³rica da Carolina Martins. 
Sua tarefa Ã© analisar {total_vagas} descriÃ§Ãµes de vagas para o cargo de "{cargo_objetivo}" 
na Ã¡rea de "{area_interesse}" e gerar um Mapa de Palavras-Chave (MPC) completo e estruturado.

METODOLOGIA CAROLINA MARTINS A SEGUIR:

1. EXTRAÃ‡ÃƒO: 
   - Identifique TODAS as competÃªncias tÃ©cnicas, ferramentas, metodologias, soft skills e qualificaÃ§Ãµes
   - Capture termos compostos (ex: "machine learning", "gestÃ£o de projetos") como unidades Ãºnicas
   - IGNORE termos genÃ©ricos como "dinÃ¢mico", "proativo", "inovador", "estratÃ©gico" quando isolados
   - FOQUE em competÃªncias ESPECÃFICAS e MENSURÃVEIS

2. CONTAGEM DE FREQUÃŠNCIA:
   - Calcule em quantas das {total_vagas} vagas cada termo aparece
   - Considere variaÃ§Ãµes do mesmo termo (SQL/MySQL, React/React.js) como ocorrÃªncias do termo principal

3. CATEGORIZAÃ‡ÃƒO em trÃªs grupos:
   - TÃ‰CNICAS: Conhecimentos especÃ­ficos da Ã¡rea, linguagens de programaÃ§Ã£o, frameworks, metodologias
   - FERRAMENTAS: Softwares, plataformas, ferramentas de produtividade (Excel, SAP, Photoshop)
   - COMPORTAMENTAIS: Soft skills REAIS e especÃ­ficas (nÃ£o adjetivos genÃ©ricos)

4. PRIORIZAÃ‡ÃƒO por frequÃªncia:
   - ESSENCIAL: presente em >60% das vagas
   - IMPORTANTE: presente em 30-60% das vagas
   - COMPLEMENTAR: presente em <30% das vagas

5. TOP 10 CAROLINA MARTINS:
   - Liste as 10 palavras-chave mais estratÃ©gicas combinando frequÃªncia e relevÃ¢ncia para o cargo

REGRAS CRÃTICAS - LEIA COM ATENÃ‡ÃƒO:

âŒ PALAVRAS PROIBIDAS (NUNCA incluir no resultado):
- Stop words: vocÃª, nosso, nossa, para, com, sem, sobre, apÃ³s, mais, menos, muito
- GenÃ©ricos sem contexto: desenvolvimento, experiÃªncia, conhecimento, habilidade, oportunidade
- Adjetivos vazios: bom, Ã³timo, excelente, dinÃ¢mico, proativo, inovador, estratÃ©gico
- Palavras de contexto: empresa, cliente, equipe, vaga, candidato, profissional, mercado
- Verbos comuns: fazer, ter, ser, estar, poder, dever, buscar, atuar

âœ… EXEMPLOS DE BOAS PALAVRAS-CHAVE:
- TÃ©cnicas: React.js, Python, Node.js, AWS, Docker, Kubernetes, SQL, MongoDB, API REST
- Ferramentas: Jira, Figma, Tableau, Power BI, Git, Postman, VS Code, Slack
- Comportamentais: gestÃ£o de conflitos, mentoria de equipes, apresentaÃ§Ã£o executiva
- Termos compostos: machine learning, cloud computing, metodologia Ã¡gil, design thinking

âš ï¸ IMPORTANTE:
- MÃ­nimo 30 palavras Ãºnicas relevantes
- Cada palavra do TOP 10 deve ser ÃšTIL para otimizar um currÃ­culo
- Preserve termos em inglÃªs quando forem padrÃ£o do mercado (nÃ£o traduza)
- Mantenha o case original (React, nÃ£o react; AWS, nÃ£o aws)

TEXTO DAS VAGAS:
{texto_vagas}

RETORNE APENAS JSON VÃLIDO, SEM TEXTO ADICIONAL!

FORMATO JSON OBRIGATÃ“RIO:
{{
  "analise_metadados": {{
    "cargo_analisado": "{cargo_objetivo}",
    "area_analisada": "{area_interesse}",
    "total_vagas": {total_vagas},
    "data_analise": "{datetime.now().strftime('%Y-%m-%d')}",
    "total_palavras_unicas": 0
  }},
  "top_10_carolina_martins": [
    {{"termo": "exemplo", "frequencia_absoluta": 85, "frequencia_percentual": 85.0, "categoria": "tecnica"}},
    ...
  ],
  "mpc_completo": {{
    "essenciais": [
      {{"termo": "...", "categoria": "tecnica|ferramentas|comportamental", "frequencia_absoluta": 65, "frequencia_percentual": 65.0}},
      ...
    ],
    "importantes": [
      ...
    ],
    "complementares": [
      ...
    ]
  }},
  "insights_adicionais": {{
    "tendencias_emergentes": ["lista de tecnologias/skills em crescimento"],
    "gaps_identificados": ["competÃªncias pouco mencionadas mas potencialmente importantes"],
    "recomendacoes": ["sugestÃµes especÃ­ficas para o candidato"]
  }}
}}"""
        
        return prompt
    
    def _chamar_claude(self, prompt: str) -> Dict[str, Any]:
        """Chama API do Claude para anÃ¡lise"""
        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=4000,
                temperature=0.3,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            # Extrair JSON da resposta
            texto_resposta = response.content[0].text
            
            # Tentar encontrar JSON na resposta
            inicio = texto_resposta.find('{')
            fim = texto_resposta.rfind('}') + 1
            
            if inicio >= 0 and fim > inicio:
                json_str = texto_resposta[inicio:fim]
                return json.loads(json_str)
            else:
                raise ValueError("JSON nÃ£o encontrado na resposta")
                
        except Exception as e:
            print(f"Erro ao chamar Claude: {e}")
            raise
    
    def _chamar_gpt4(self, prompt: str) -> Dict[str, Any]:
        """Chama API do GPT-4 para anÃ¡lise"""
        try:
            response = self.openai_client.ChatCompletion.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": "VocÃª Ã© um especialista em anÃ¡lise de vagas e extraÃ§Ã£o de palavras-chave. Sempre retorne JSON vÃ¡lido."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=4000,
                response_format={"type": "json_object"}
            )
            
            texto_resposta = response.choices[0].message.content
            return json.loads(texto_resposta)
            
        except Exception as e:
            print(f"Erro ao chamar GPT-4: {e}")
            raise
    
    def _chamar_gemini(self, prompt: str) -> Dict[str, Any]:
        """Chama API do Gemini 2.5 Pro para anÃ¡lise"""
        try:
            # ConfiguraÃ§Ã£o otimizada para Gemini Pro
            response = self.gemini_model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.3,
                    "max_output_tokens": 4000,  # Limite seguro
                    # Removido response_mime_type que pode causar problemas
                }
            )
            
            # Com response_mime_type, o Gemini 2.5 Pro jÃ¡ retorna JSON estruturado
            texto_resposta = response.text
            
            # Debug: imprimir primeiros caracteres da resposta
            print(f"DEBUG: Resposta Gemini (primeiros 200 chars): {texto_resposta[:200]}")
            
            try:
                # Tenta parse direto jÃ¡ que configuramos JSON output
                return json.loads(texto_resposta)
            except json.JSONDecodeError as e:
                print(f"DEBUG: Erro de JSON na posiÃ§Ã£o {e.pos}: {e.msg}")
                print(f"DEBUG: Contexto do erro: ...{texto_resposta[max(0, e.pos-50):e.pos+50]}...")
                
                # Tenta limpar JSON comum problemas
                # Remove possÃ­veis caracteres extras no inÃ­cio/fim
                texto_limpo = texto_resposta.strip()
                
                # Remove possÃ­veis markdown code blocks
                if texto_limpo.startswith("```json"):
                    texto_limpo = texto_limpo[7:]
                if texto_limpo.startswith("```"):
                    texto_limpo = texto_limpo[3:]
                if texto_limpo.endswith("```"):
                    texto_limpo = texto_limpo[:-3]
                
                # Tenta novamente apÃ³s limpeza
                try:
                    return json.loads(texto_limpo.strip())
                except json.JSONDecodeError:
                    # Fallback para extraÃ§Ã£o manual se necessÃ¡rio
                    inicio = texto_resposta.find('{')
                    fim = texto_resposta.rfind('}') + 1
                    
                    if inicio >= 0 and fim > inicio:
                        json_str = texto_resposta[inicio:fim]
                        # Ãšltima tentativa com JSON extraÃ­do
                        try:
                            return json.loads(json_str)
                        except json.JSONDecodeError as e2:
                            print(f"DEBUG: Falha final no parse. Erro: {e2}")
                            print(f"DEBUG: JSON extraÃ­do tem {len(json_str)} caracteres")
                            # Salvar resposta para debug
                            with open('gemini_response_debug.json', 'w', encoding='utf-8') as f:
                                f.write(texto_resposta)
                            print("DEBUG: Resposta completa salva em gemini_response_debug.json")
                            raise ValueError(f"JSON invÃ¡lido na resposta: {e2}")
                    else:
                        raise ValueError("JSON nÃ£o encontrado na resposta")
                
        except Exception as e:
            print(f"Erro ao chamar Gemini: {e}")
            raise
    
    def _processar_resultado_ia(self, resultado: Dict[str, Any], modelo: str) -> Dict[str, Any]:
        """Processa e valida resultado da IA"""
        # Adicionar metadados
        resultado['analise_metadados']['modelo_ia_usado'] = modelo
        
        # Calcular total de palavras Ãºnicas
        todas_palavras = set()
        
        if 'mpc_completo' in resultado:
            for nivel in ['essenciais', 'importantes', 'complementares']:
                if nivel in resultado['mpc_completo']:
                    for palavra in resultado['mpc_completo'][nivel]:
                        todas_palavras.add(palavra['termo'])
        
        resultado['analise_metadados']['total_palavras_unicas'] = len(todas_palavras)
        
        # Validar top 10
        if 'top_10_carolina_martins' not in resultado:
            resultado['top_10_carolina_martins'] = []
        
        return resultado