"""
Extrator de Palavras-Chave via IA - Sistema HELIO
Abordagem "Pura IA" seguindo metodologia Carolina Martins
Usa LLMs com grande janela de contexto para an√°lise completa
"""

import os
import json
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import openai
from anthropic import Anthropic
import google.generativeai as genai
from collections import Counter
from dotenv import load_dotenv

# Garantir que as vari√°veis de ambiente sejam carregadas
load_dotenv()

class AIKeywordExtractor:
    """
    Extrator que envia todas as descri√ß√µes de vagas para um LLM
    para an√°lise completa e extra√ß√£o inteligente de palavras-chave
    """
    
    def __init__(self):
        # Configurar clientes de IA
        self.anthropic_client = None
        self.openai_client = None
        self.gemini_model = None
        
        # Claude (200k tokens de contexto)
        if os.getenv('ANTHROPIC_API_KEY'):
            try:
                self.anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
                print("‚úÖ Claude client inicializado com sucesso")
            except Exception as e:
                print(f"‚ùå Erro ao inicializar Claude: {e}")
                self.anthropic_client = None
        
        # OpenAI GPT-4 Turbo (128k tokens)
        if os.getenv('OPENAI_API_KEY'):
            try:
                openai.api_key = os.getenv('OPENAI_API_KEY')
                self.openai_client = openai
                print("‚úÖ OpenAI client inicializado com sucesso")
            except Exception as e:
                print(f"‚ùå Erro ao inicializar OpenAI: {e}")
                self.openai_client = None
        
        # Google Gemini 2.5 Flash (2M tokens input)
        if os.getenv('GOOGLE_API_KEY'):
            try:
                genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
                # Usando Gemini 2.5 Flash (2025)
                self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
                print("‚úÖ Gemini client inicializado com sucesso")
            except Exception as e:
                print(f"‚ùå Erro ao inicializar Gemini: {e}")
                self.gemini_model = None
    
    async def extrair_palavras_chave_ia(
        self, 
        vagas: List[Dict[str, Any]], 
        cargo_objetivo: str,
        area_interesse: str,
        callback_progresso: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        Extrai palavras-chave usando an√°lise completa via IA
        
        Args:
            vagas: Lista de dicion√°rios com descri√ß√µes de vagas
            cargo_objetivo: Cargo alvo do usu√°rio
            area_interesse: √Årea de interesse
            callback_progresso: Fun√ß√£o para reportar progresso
            
        Returns:
            Dict com an√°lise completa incluindo top 10 e categoriza√ß√£o
        """
        
        if callback_progresso:
            await callback_progresso("Preparando descri√ß√µes para an√°lise IA...")
        
        # Preparar texto agregado
        texto_agregado = self._preparar_texto_vagas(vagas)
        total_vagas = len(vagas)
        
        print(f"\nü§ñ === EXTRA√á√ÉO VIA IA ===")
        print(f"üìä Total de vagas: {total_vagas}")
        print(f"üìù Tamanho do texto: {len(texto_agregado)} caracteres")
        print(f"üéØ Cargo: {cargo_objetivo}")
        print(f"üè¢ √Årea: {area_interesse}")
        
        # Criar prompt sofisticado
        prompt = self._criar_prompt_extracao(
            texto_agregado, 
            cargo_objetivo, 
            area_interesse, 
            total_vagas
        )
        
        if callback_progresso:
            await callback_progresso("Analisando vagas com IA (isso pode levar 30-60 segundos)...")
        
        # Escolher modelo baseado em disponibilidade e tamanho
        resultado = None
        modelo_usado = None
        
        try:
            print(f"\nüîç Verificando modelos dispon√≠veis:")
            print(f"   - Gemini configurado: {self.gemini_model is not None}")
            print(f"   - Claude configurado: {self.anthropic_client is not None}")
            print(f"   - GPT-4 configurado: {self.openai_client is not None}")
            print(f"   - Tamanho do texto: {len(texto_agregado)} caracteres")
            
            # Prefer√™ncia: Gemini 2.5 Flash (2M tokens) > Claude (200k) > GPT-4 (128k)
            if self.gemini_model:
                try:
                    if callback_progresso:
                        await callback_progresso("Usando Google Gemini 2.5 Flash...")
                    print(f"‚úÖ Chamando Gemini 2.5 Flash...")
                    resultado = self._chamar_gemini(prompt)
                    modelo_usado = "gemini-2.5-flash"
                except Exception as gemini_error:
                    print(f"‚ö†Ô∏è Gemini falhou: {gemini_error}")
                    print(f"üîÑ Tentando com Claude como fallback...")
                    
                    # Fallback para Claude se Gemini falhar
                    if self.anthropic_client and len(texto_agregado) < 180000:
                        if callback_progresso:
                            await callback_progresso("Usando Claude 3 Sonnet (fallback)...")
                        resultado = self._chamar_claude(prompt)
                        modelo_usado = "claude-3-sonnet"
                    else:
                        raise gemini_error
                
            elif self.anthropic_client and len(texto_agregado) < 180000:
                if callback_progresso:
                    await callback_progresso("Usando Claude 3 Sonnet...")
                print(f"‚úÖ Chamando Claude 3 Sonnet...")
                resultado = self._chamar_claude(prompt)
                modelo_usado = "claude-3-sonnet"
                
            elif self.openai_client and len(texto_agregado) < 100000:
                if callback_progresso:
                    await callback_progresso("Usando GPT-4 Turbo...")
                print(f"‚úÖ Chamando GPT-4 Turbo...")
                resultado = self._chamar_gpt4(prompt)
                modelo_usado = "gpt-4-turbo"
                
            else:
                raise Exception("Texto muito grande ou nenhuma API configurada. Por favor, configure pelo menos uma API key (GOOGLE_API_KEY, ANTHROPIC_API_KEY ou OPENAI_API_KEY)")
                
        except Exception as e:
            print(f"‚ùå Erro na an√°lise IA: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            if callback_progresso:
                await callback_progresso(f"Erro: {str(e)}")
            
            # N√ÉO usar fallback - √© melhor falhar do que dar resultado ruim
            erro_msg = f"N√£o foi poss√≠vel analisar as vagas com IA. {str(e)}"
            raise Exception(erro_msg)
        
        if callback_progresso:
            await callback_progresso("Processando resultados da IA...")
        
        # Processar e validar resultado
        resultado_final = self._processar_resultado_ia(resultado, modelo_usado)
        
        print(f"\n‚úÖ An√°lise conclu√≠da com {modelo_usado}")
        print(f"üîù Top 10 palavras: {len(resultado_final.get('top_10_carolina_martins', []))}")
        print(f"üìä Total palavras categorizadas: {resultado_final.get('total_palavras_unicas', 0)}")
        
        return resultado_final
    
    def _preparar_texto_vagas(self, vagas: List[Dict[str, Any]]) -> str:
        """Prepara texto agregado das vagas com separadores claros"""
        textos = []
        
        for i, vaga in enumerate(vagas, 1):
            titulo = vaga.get('titulo', 'Sem t√≠tulo')
            empresa = vaga.get('empresa', 'Empresa n√£o informada')
            descricao = vaga.get('descricao', '')
            
            # Formato estruturado para melhor compreens√£o da IA
            texto_vaga = f"""
--- VAGA {i} ---
T√≠tulo: {titulo}
Empresa: {empresa}
Descri√ß√£o:
{descricao}
--- FIM VAGA {i} ---
"""
            textos.append(texto_vaga)
        
        return '\n'.join(textos)
    
    def _criar_prompt_extracao(
        self, 
        texto_vagas: str, 
        cargo_objetivo: str,
        area_interesse: str,
        total_vagas: int
    ) -> str:
        """Cria prompt sofisticado para extra√ß√£o via IA"""
        
        prompt = f"""Voc√™ √© o Agente de Palavras-chave do m√©todo Carreira Mete√≥rica da Carolina Martins. 
Sua tarefa √© analisar {total_vagas} descri√ß√µes de vagas para o cargo de "{cargo_objetivo}" 
na √°rea de "{area_interesse}" e gerar um Mapa de Palavras-Chave (MPC) completo e estruturado.

METODOLOGIA CAROLINA MARTINS A SEGUIR:

1. EXTRA√á√ÉO: 
   - Identifique TODAS as compet√™ncias t√©cnicas, ferramentas, metodologias, soft skills e qualifica√ß√µes
   - Capture termos compostos (ex: "machine learning", "gest√£o de projetos") como unidades √∫nicas
   - IGNORE termos gen√©ricos como "din√¢mico", "proativo", "inovador", "estrat√©gico" quando isolados
   - FOQUE em compet√™ncias ESPEC√çFICAS e MENSUR√ÅVEIS

2. CONTAGEM DE FREQU√äNCIA:
   - Calcule em quantas das {total_vagas} vagas cada termo aparece
   - Considere varia√ß√µes do mesmo termo (SQL/MySQL, React/React.js) como ocorr√™ncias do termo principal

3. CATEGORIZA√á√ÉO em tr√™s grupos:
   - T√âCNICAS: Conhecimentos espec√≠ficos da √°rea, linguagens de programa√ß√£o, frameworks, metodologias
   - FERRAMENTAS: Softwares, plataformas, ferramentas de produtividade (Excel, SAP, Photoshop)
   - COMPORTAMENTAIS: Soft skills REAIS e espec√≠ficas (n√£o adjetivos gen√©ricos)

4. PRIORIZA√á√ÉO por frequ√™ncia:
   - ESSENCIAL: presente em >60% das vagas
   - IMPORTANTE: presente em 30-60% das vagas
   - COMPLEMENTAR: presente em <30% das vagas

5. TOP 10 CAROLINA MARTINS:
   - Liste as 10 palavras-chave mais estrat√©gicas combinando frequ√™ncia e relev√¢ncia para o cargo

REGRAS CR√çTICAS - LEIA COM ATEN√á√ÉO:

‚ùå PALAVRAS PROIBIDAS (NUNCA incluir no resultado):
- Stop words: voc√™, nosso, nossa, para, com, sem, sobre, ap√≥s, mais, menos, muito
- Gen√©ricos sem contexto: desenvolvimento, experi√™ncia, conhecimento, habilidade, oportunidade
- Adjetivos vazios: bom, √≥timo, excelente, din√¢mico, proativo, inovador, estrat√©gico
- Palavras de contexto: empresa, cliente, equipe, vaga, candidato, profissional, mercado
- Verbos comuns: fazer, ter, ser, estar, poder, dever, buscar, atuar

‚úÖ EXEMPLOS DE BOAS PALAVRAS-CHAVE:
- T√©cnicas: React.js, Python, Node.js, AWS, Docker, Kubernetes, SQL, MongoDB, API REST
- Ferramentas: Jira, Figma, Tableau, Power BI, Git, Postman, VS Code, Slack
- Comportamentais: gest√£o de conflitos, mentoria de equipes, apresenta√ß√£o executiva
- Termos compostos: machine learning, cloud computing, metodologia √°gil, design thinking

‚ö†Ô∏è IMPORTANTE:
- M√≠nimo 30 palavras √∫nicas relevantes
- Cada palavra do TOP 10 deve ser √öTIL para otimizar um curr√≠culo
- Preserve termos em ingl√™s quando forem padr√£o do mercado (n√£o traduza)
- Mantenha o case original (React, n√£o react; AWS, n√£o aws)

TEXTO DAS VAGAS:
{texto_vagas}

RETORNE APENAS JSON V√ÅLIDO, SEM TEXTO ADICIONAL!

FORMATO JSON OBRIGAT√ìRIO:
{{
  "analise_metadados": {{
    "cargo_analisado": "{cargo_objetivo}",
    "area_analisada": "{area_interesse}",
    "total_vagas": {total_vagas},
    "data_analise": "{datetime.now().strftime('%Y-%m-%d')}",
    "total_palavras_unicas": 0
  }},
  "top_10_carolina_martins": [
    {{"termo": "exemplo", "frequencia_absoluta": 85, "frequencia_percentual": 85.0, "categoria": "tecnica"}},
    ...
  ],
  "mpc_completo": {{
    "essenciais": [
      {{"termo": "...", "categoria": "tecnica|ferramentas|comportamental", "frequencia_absoluta": 65, "frequencia_percentual": 65.0}},
      ...
    ],
    "importantes": [
      ...
    ],
    "complementares": [
      ...
    ]
  }},
  "insights_adicionais": {{
    "tendencias_emergentes": ["lista de tecnologias/skills em crescimento"],
    "gaps_identificados": ["compet√™ncias pouco mencionadas mas potencialmente importantes"],
    "recomendacoes": ["sugest√µes espec√≠ficas para o candidato"]
  }}
}}"""
        
        return prompt
    
    def _chamar_claude(self, prompt: str) -> Dict[str, Any]:
        """Chama API do Claude para an√°lise"""
        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=4000,
                temperature=0.3,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            # Extrair JSON da resposta
            texto_resposta = response.content[0].text
            
            # Tentar encontrar JSON na resposta
            inicio = texto_resposta.find('{')
            fim = texto_resposta.rfind('}') + 1
            
            if inicio >= 0 and fim > inicio:
                json_str = texto_resposta[inicio:fim]
                return json.loads(json_str)
            else:
                raise ValueError("JSON n√£o encontrado na resposta")
                
        except Exception as e:
            print(f"Erro ao chamar Claude: {e}")
            raise
    
    def _chamar_gpt4(self, prompt: str) -> Dict[str, Any]:
        """Chama API do GPT-4 para an√°lise"""
        try:
            response = self.openai_client.ChatCompletion.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": "Voc√™ √© um especialista em an√°lise de vagas e extra√ß√£o de palavras-chave. Sempre retorne JSON v√°lido."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=4000,
                response_format={"type": "json_object"}
            )
            
            texto_resposta = response.choices[0].message.content
            return json.loads(texto_resposta)
            
        except Exception as e:
            print(f"Erro ao chamar GPT-4: {e}")
            raise
    
    def _chamar_gemini(self, prompt: str) -> Dict[str, Any]:
        """Chama API do Gemini 2.5 Flash para an√°lise"""
        try:
            # Configura√ß√£o otimizada para Gemini Pro
            response = self.gemini_model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.3,
                    "max_output_tokens": 4000,  # Limite seguro
                    # Removido response_mime_type que pode causar problemas
                }
            )
            
            # Com response_mime_type, o Gemini 2.5 Flash j√° retorna JSON estruturado
            texto_resposta = response.text
            
            # Debug: imprimir primeiros caracteres da resposta
            print(f"DEBUG: Resposta Gemini (primeiros 200 chars): {texto_resposta[:200]}")
            
            try:
                # Tenta parse direto j√° que configuramos JSON output
                return json.loads(texto_resposta)
            except json.JSONDecodeError as e:
                print(f"DEBUG: Erro de JSON na posi√ß√£o {e.pos}: {e.msg}")
                print(f"DEBUG: Contexto do erro: ...{texto_resposta[max(0, e.pos-50):e.pos+50]}...")
                
                # Tenta limpar JSON comum problemas
                # Remove poss√≠veis caracteres extras no in√≠cio/fim
                texto_limpo = texto_resposta.strip()
                
                # Remove poss√≠veis markdown code blocks
                if texto_limpo.startswith("```json"):
                    texto_limpo = texto_limpo[7:]
                if texto_limpo.startswith("```"):
                    texto_limpo = texto_limpo[3:]
                if texto_limpo.endswith("```"):
                    texto_limpo = texto_limpo[:-3]
                
                # Tenta novamente ap√≥s limpeza
                try:
                    return json.loads(texto_limpo.strip())
                except json.JSONDecodeError:
                    # Fallback para extra√ß√£o manual se necess√°rio
                    inicio = texto_resposta.find('{')
                    fim = texto_resposta.rfind('}') + 1
                    
                    if inicio >= 0 and fim > inicio:
                        json_str = texto_resposta[inicio:fim]
                        # √öltima tentativa com JSON extra√≠do
                        try:
                            return json.loads(json_str)
                        except json.JSONDecodeError as e2:
                            print(f"DEBUG: Falha final no parse. Erro: {e2}")
                            print(f"DEBUG: JSON extra√≠do tem {len(json_str)} caracteres")
                            # Salvar resposta para debug
                            with open('gemini_response_debug.json', 'w', encoding='utf-8') as f:
                                f.write(texto_resposta)
                            print("DEBUG: Resposta completa salva em gemini_response_debug.json")
                            raise ValueError(f"JSON inv√°lido na resposta: {e2}")
                    else:
                        raise ValueError("JSON n√£o encontrado na resposta")
                
        except Exception as e:
            print(f"Erro ao chamar Gemini: {e}")
            raise
    
    def _processar_resultado_ia(self, resultado: Dict[str, Any], modelo: str) -> Dict[str, Any]:
        """Processa e valida resultado da IA"""
        # Adicionar metadados
        resultado['analise_metadados']['modelo_ia_usado'] = modelo
        
        # Calcular total de palavras √∫nicas
        todas_palavras = set()
        
        if 'mpc_completo' in resultado:
            for nivel in ['essenciais', 'importantes', 'complementares']:
                if nivel in resultado['mpc_completo']:
                    for palavra in resultado['mpc_completo'][nivel]:
                        todas_palavras.add(palavra['termo'])
        
        resultado['analise_metadados']['total_palavras_unicas'] = len(todas_palavras)
        
        # Validar top 10
        if 'top_10_carolina_martins' not in resultado:
            resultado['top_10_carolina_martins'] = []
        
        return resultado