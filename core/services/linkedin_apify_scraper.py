"""
LinkedIn Scraper usando Apify - Alternativa profissional ao Selenium
Coleta real de vagas do LinkedIn usando a infraestrutura da Apify
"""

import os
import time
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
import requests
from dotenv import load_dotenv

load_dotenv()

class LinkedInApifyScraper:
    """
    Coleta vagas do LinkedIn usando Apify Actor
    Mais confiÃ¡vel e escalÃ¡vel que Selenium
    """
    
    def __init__(self):
        """
        Inicializa o scraper do LinkedIn via Apify
        Usando curious_coder~linkedin-jobs-scraper otimizado
        """
        self.apify_token = os.getenv('APIFY_API_TOKEN')
        self.base_url = "https://api.apify.com/v2"
        self.actor_id = "curious_coder~linkedin-jobs-scraper"  # âœ… Actor correto e gratuito
        
        if not self.apify_token:
            print("âš ï¸  APIFY_API_TOKEN nÃ£o encontrado. Usando dados de fallback.")
    
    def coletar_vagas_linkedin(
        self, 
        cargo: str, 
        localizacao: str = "SÃ£o Paulo, Brazil",
        limite: int = 800  # ğŸ”¥ PADRÃƒO ALTO: 800 vagas
    ) -> List[Dict[str, Any]]:
        """
        Coleta vagas do LinkedIn usando Apify (aproveita TODAS as vagas disponÃ­veis)
        
        Args:
            cargo: Cargo/posiÃ§Ã£o desejada
            localizacao: LocalizaÃ§Ã£o para busca  
            limite: MÃ¡ximo de vagas (padrÃ£o: 800, usa todas se Apify trouxer mais)
        """
        
        if not self.apify_token:
            print("ğŸš¨ Token Apify nÃ£o configurado. Usando fallback.")
            return self._dados_fallback_linkedin()
        
        try:
            # ğŸ”¥ URL OTIMIZADA: Filtro Ãºltimos 7 dias para relevÃ¢ncia
            search_url = f"https://www.linkedin.com/jobs/search/?keywords={cargo}&location={localizacao}&f_TPR=r604800"
            
            # ğŸ¯ INPUT MÃXIMO: Deixa Apify buscar o mÃ¡ximo possÃ­vel
            input_data = {
                "urls": [search_url],
                "numberOfJobsNeeded": 20000,  # ğŸš€ MÃXIMO POSSÃVEL! 
                "scrapeCompanyDetails": True,
                "proxy": {
                    "useApifyProxy": True,
                    "apifyProxyGroups": ["RESIDENTIAL"]
                },
                "timeout": 600,  # ğŸ• 10 minutos - mais tempo para mais vagas
                "maxConcurrency": 3  # Aumenta concorrÃªncia
            }
            
            print(f"ğŸš€ Buscando MÃXIMO de vagas: {cargo} em {localizacao}")
            print(f"ğŸ“Š Limite do usuÃ¡rio: {limite} | Apify buscarÃ¡: atÃ© 20.000!")
            
            # Iniciar execuÃ§Ã£o
            run_response = requests.post(
                f"{self.base_url}/acts/{self.actor_id}/runs",
                headers={
                    "Authorization": f"Bearer {self.apify_token}",
                    "Content-Type": "application/json"
                },
                json=input_data,
                timeout=30
            )
            
            if run_response.status_code != 201:
                print(f"âŒ Erro ao iniciar scraping: {run_response.status_code}")
                return self._dados_fallback_linkedin()
            
            run_id = run_response.json()["data"]["id"]
            print(f"âœ… Scraping iniciado - ID: {run_id}")
            
            # ğŸ• AGUARDAR com paciÃªncia para MAIS vagas
            max_attempts = 40  # ~7 minutos mÃ¡ximo (mais tempo = mais vagas)
            attempt = 0
            
            while attempt < max_attempts:
                time.sleep(10)  # Check a cada 10 segundos
                attempt += 1
                
                status_response = requests.get(
                    f"{self.base_url}/actor-runs/{run_id}",
                    headers={"Authorization": f"Bearer {self.apify_token}"},
                    timeout=10
                )
                
                if status_response.status_code == 200:
                    status = status_response.json()["data"]["status"]
                    
                    # ğŸ“Š Log progresso a cada minuto
                    if attempt % 6 == 0:  # A cada 6 checks = 1 minuto
                        print(f"â³ Aguardando... {attempt//6}min | Status: {status}")
                    
                    if status == "SUCCEEDED":
                        print(f"ğŸ‰ Scraping concluÃ­do em {attempt//6}min!")
                        break
                    elif status in ["FAILED", "ABORTED", "TIMED-OUT"]:
                        print(f"âŒ Scraping falhou: {status}")
                        return self._dados_fallback_linkedin()
                else:
                    print(f"âš ï¸ Erro ao verificar status: {status_response.status_code}")
            
            if attempt >= max_attempts:
                print("â° Timeout: Mas vamos tentar baixar o que conseguiu...")
                # ğŸ¯ Mesmo com timeout, tenta baixar resultados parciais
            
            # ğŸ“¥ BAIXAR TODOS OS RESULTADOS
            results_response = requests.get(
                f"{self.base_url}/datasets/{run_id}/items",
                headers={"Authorization": f"Bearer {self.apify_token}"},
                timeout=60  # Mais tempo para download
            )
            
            if results_response.status_code != 200:
                print(f"âŒ Erro ao baixar resultados: {results_response.status_code}")
                return self._dados_fallback_linkedin()
            
            raw_jobs = results_response.json()
            total_encontradas = len(raw_jobs)
            
            print(f"ğŸŠ SUCESSO! {total_encontradas} vagas encontradas pelo Apify!")
            
            # ğŸ¯ ESTRATÃ‰GIA INTELIGENTE DE LIMITE:
            if total_encontradas <= limite:
                # Se Apify trouxe menos que o limite, USA TODAS!
                vagas_finais = raw_jobs
                print(f"âœ… Usando TODAS as {total_encontradas} vagas (menor que limite {limite})")
            else:
                # Se Apify trouxe mais, respeita o limite do usuÃ¡rio
                vagas_finais = raw_jobs[:limite] 
                print(f"ğŸ“Š Limitando para {limite} vagas (Apify trouxe {total_encontradas})")
            
            # ğŸ”§ PROCESSAR RESULTADOS
            processed_jobs = []
            for i, job_data in enumerate(vagas_finais):
                try:
                    processed_job = {
                        "titulo": job_data.get("jobTitle", "TÃ­tulo nÃ£o informado"),
                        "empresa": job_data.get("companyName", "Empresa nÃ£o informada"),
                        "localizacao": job_data.get("location", localizacao),
                        "descricao": job_data.get("description", "DescriÃ§Ã£o nÃ£o disponÃ­vel")[:500],
                        "link": job_data.get("jobUrl", "#"),
                        "data_publicacao": job_data.get("postedAt", "NÃ£o informado"),
                        "tipo_contrato": job_data.get("jobType", "NÃ£o especificado"),
                        "nivel_experiencia": job_data.get("seniorityLevel", "NÃ£o especificado"),
                        "salario": job_data.get("salary", "NÃ£o informado"),
                        "fonte": "LinkedIn (Apify)"
                    }
                    processed_jobs.append(processed_job)
                except Exception as e:
                    print(f"âš ï¸ Erro ao processar vaga {i+1}: {e}")
                    continue
            
            print(f"ğŸ‰ RESULTADO FINAL: {len(processed_jobs)} vagas processadas!")
            print(f"ğŸ“ˆ Taxa de sucesso: {len(processed_jobs)/len(vagas_finais)*100:.1f}%")
            return processed_jobs
            
        except Exception as e:
            print(f"ğŸš¨ Erro no scraping LinkedIn: {e}")
            return self._dados_fallback_linkedin()
    
    def _processar_resultados_apify(self, items: List[Dict], cargo_pesquisado: str) -> List[Dict[str, Any]]:
        """
        Processa os resultados do Apify para o formato padrÃ£o
        Adaptado para o actor bebity/linkedin-jobs-scraper
        """
        vagas_processadas = []
        
        for item in items:
            try:
                # O actor bebity retorna campos neste formato
                vaga = {
                    "titulo": item.get('title', item.get('jobTitle', 'TÃ­tulo nÃ£o disponÃ­vel')),
                    "empresa": item.get('companyName', item.get('company', 'Empresa nÃ£o informada')),
                    "localizacao": item.get('location', 'Local nÃ£o informado'),
                    "descricao": item.get('description', item.get('jobDescription', 'DescriÃ§Ã£o nÃ£o disponÃ­vel')),
                    "fonte": "linkedin_apify",
                    "url": item.get('link', item.get('jobUrl', '')),
                    "data_coleta": datetime.now().isoformat(),
                    "cargo_pesquisado": cargo_pesquisado,
                    "data_publicacao": item.get('postedTime', item.get('publishedAt', '')),
                    "salario": item.get('salary', ''),
                    "tipo_emprego": item.get('contractType', item.get('employmentType', '')),
                    "nivel_experiencia": item.get('seniorityLevel', item.get('experienceLevel', '')),
                    "empresa_logo": item.get('companyLogo', ''),
                    "empresa_linkedin": item.get('companyLink', item.get('companyUrl', '')),
                    "apify_real": True  # Marca como dados reais do Apify
                }
                
                vagas_processadas.append(vaga)
                
            except Exception as e:
                print(f"âš ï¸  Erro ao processar vaga: {e}")
                continue
        
        print(f"âœ… Processadas {len(vagas_processadas)} vagas do LinkedIn via Apify")
        return vagas_processadas
    
    def _fallback_linkedin_data(self, cargo: str, localizacao: str, limite: int) -> List[Dict[str, Any]]:
        """
        Dados de fallback quando Apify nÃ£o estÃ¡ disponÃ­vel
        Usa API pÃºblica limitada do LinkedIn
        """
        print("ğŸ”„ Usando fallback: tentando API pÃºblica do LinkedIn...")
        
        vagas = []
        
        try:
            # Tenta usar endpoint pÃºblico (muito limitado)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            query = cargo.replace(' ', '%20')
            location = localizacao.replace(' ', '%20').replace(',', '%2C')
            
            url = f"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={query}&location={location}&start=0"
            
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                # Parse HTML response (limitado)
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                
                job_cards = soup.find_all('div', class_='job-search-card')[:limite]
                
                for card in job_cards:
                    titulo = card.find('h3', class_='job-search-card__title')
                    empresa = card.find('h4', class_='job-search-card__company-name')
                    local = card.find('span', class_='job-search-card__location')
                    
                    vaga = {
                        "titulo": titulo.text.strip() if titulo else cargo,
                        "empresa": empresa.text.strip() if empresa else "Empresa LinkedIn",
                        "localizacao": local.text.strip() if local else localizacao,
                        "descricao": f"Vaga para {cargo} no LinkedIn. Acesse o LinkedIn para mais detalhes.",
                        "fonte": "linkedin_public_api",
                        "url": "https://www.linkedin.com/jobs/",
                        "data_coleta": datetime.now().isoformat(),
                        "cargo_pesquisado": cargo,
                        "api_limitada": True
                    }
                    
                    vagas.append(vaga)
                
                print(f"âœ… Coletadas {len(vagas)} vagas via API pÃºblica")
                
        except Exception as e:
            print(f"âŒ Fallback tambÃ©m falhou: {e}")
        
        return vagas
    
    def _construir_url_busca(self, cargo: str, localizacao: str) -> str:
        """
        ConstrÃ³i URL de busca do LinkedIn Jobs
        """
        # Formatar parÃ¢metros para URL
        keywords = cargo.replace(' ', '%20')
        location = localizacao.replace(' ', '%20').replace(',', '%2C')
        
        # URL padrÃ£o de busca do LinkedIn Jobs
        url = f"https://www.linkedin.com/jobs/search/?keywords={keywords}&location={location}"
        
        print(f"   URL construÃ­da: {url}")
        return url
    
    def verificar_credenciais(self) -> bool:
        """
        Verifica se as credenciais do Apify estÃ£o vÃ¡lidas
        """
        if not self.apify_token:
            return False
        
        try:
            url = f"{self.base_url}/users/me?token={self.apify_token}"
            response = requests.get(url)
            return response.status_code == 200
        except:
            return False
    
    def iniciar_execucao_apify(self, cargo: str, localizacao: str, limite: int = 800) -> tuple:
        """
        Inicia execuÃ§Ã£o no Apify e retorna (run_id, dataset_id) para streaming
        """
        
        if not self.apify_token:
            return None, None
        
        try:
            # URL de busca otimizada
            search_url = f"https://www.linkedin.com/jobs/search/?keywords={cargo}&location={localizacao}&f_TPR=r604800"
            
            # ParÃ¢metros para o actor
            actor_input = {
                "urls": [search_url],
                "numberOfJobsNeeded": limite,
                "scrapeCompanyDetails": True,
                "proxy": {
                    "useApifyProxy": True,
                    "apifyProxyGroups": ["RESIDENTIAL"]
                }
            }
            
            # Iniciar execuÃ§Ã£o
            run_response = requests.post(
                f"{self.base_url}/acts/{self.actor_id}/runs",
                params={"token": self.apify_token},
                json=actor_input,
                timeout=30
            )
            
            if run_response.status_code == 201:
                run_data = run_response.json()["data"]
                run_id = run_data["id"]
                dataset_id = run_data["defaultDatasetId"]
                
                print(f"ğŸš€ Run iniciado: {run_id}, Dataset: {dataset_id}")
                return run_id, dataset_id
            else:
                print(f"âŒ Erro ao iniciar run: {run_response.status_code}")
                return None, None
                
        except Exception as e:
            print(f"âŒ Erro na execuÃ§Ã£o Apify: {e}")
            return None, None
    
    def verificar_status_run(self, run_id: str) -> str:
        """
        Verifica status de um run especÃ­fico
        """
        
        if not self.apify_token or not run_id:
            return "UNKNOWN"
        
        try:
            response = requests.get(
                f"{self.base_url}/actor-runs/{run_id}",
                params={"token": self.apify_token},
                timeout=10
            )
            
            if response.status_code == 200:
                status = response.json()["data"]["status"]
                return status
            else:
                return "ERROR"
                
        except Exception as e:
            print(f"âŒ Erro ao verificar status: {e}")
            return "ERROR"
    
    def contar_resultados_dataset(self, dataset_id: str) -> int:
        """
        Conta quantos itens estÃ£o no dataset atualmente
        """
        
        if not self.apify_token or not dataset_id:
            return 0
        
        try:
            response = requests.get(
                f"{self.base_url}/datasets/{dataset_id}",
                params={"token": self.apify_token},
                timeout=10
            )
            
            if response.status_code == 200:
                item_count = response.json()["data"]["itemCount"]
                return item_count
            else:
                return 0
                
        except Exception as e:
            print(f"âŒ Erro ao contar resultados: {e}")
            return 0
    
    def obter_resultados_parciais(self, dataset_id: str, offset: int, limit: int) -> List[Dict]:
        """
        ObtÃ©m resultados parciais do dataset (offset atÃ© limit)
        """
        
        if not self.apify_token or not dataset_id:
            return []
        
        try:
            params = {
                "token": self.apify_token,
                "format": "json",
                "clean": "true",
                "offset": offset,
                "limit": limit - offset
            }
            
            response = requests.get(
                f"{self.base_url}/datasets/{dataset_id}/items",
                params=params,
                timeout=30
            )
            
            if response.status_code == 200:
                raw_jobs = response.json()
                
                # Processar vagas
                vagas_processadas = []
                for job in raw_jobs:
                    vaga_processada = self._processar_vaga_linkedin(job)
                    if vaga_processada:
                        vagas_processadas.append(vaga_processada)
                
                return vagas_processadas
            else:
                return []
                
        except Exception as e:
            print(f"âŒ Erro ao obter resultados parciais: {e}")
            return []
    
    def obter_todos_resultados(self, dataset_id: str) -> List[Dict]:
        """
        ObtÃ©m todos os resultados finais do dataset
        """
        
        if not self.apify_token or not dataset_id:
            return []
        
        try:
            params = {
                "token": self.apify_token,
                "format": "json",
                "clean": "true"
            }
            
            response = requests.get(
                f"{self.base_url}/datasets/{dataset_id}/items",
                params=params,
                timeout=60
            )
            
            if response.status_code == 200:
                raw_jobs = response.json()
                
                # Processar todas as vagas
                vagas_processadas = []
                for job in raw_jobs:
                    vaga_processada = self._processar_vaga_linkedin(job)
                    if vaga_processada:
                        vagas_processadas.append(vaga_processada)
                
                print(f"âœ… Total processado: {len(vagas_processadas)} vagas")
                return vagas_processadas
            else:
                return []
                
        except Exception as e:
            print(f"âŒ Erro ao obter todos os resultados: {e}")
            return []


# Exemplo de uso
if __name__ == "__main__":
    scraper = LinkedInApifyScraper()
    
    # Verifica credenciais
    if scraper.verificar_credenciais():
        print("âœ… Apify configurado corretamente!")
    else:
        print("âŒ Configure APIFY_API_TOKEN no .env")
        print("ğŸ“Œ Crie uma conta gratuita em: https://apify.com")
        print("ğŸ“Œ Copie seu API token e adicione ao .env")
    
    # Testa coleta
    vagas = scraper.coletar_vagas_linkedin(
        cargo="Python Developer",
        localizacao="SÃ£o Paulo, Brazil",
        limite=10
    )
    
    print(f"\nğŸ“Š Total de vagas coletadas: {len(vagas)}")
    
    for i, vaga in enumerate(vagas[:3], 1):
        print(f"\n--- Vaga {i} ---")
        print(f"TÃ­tulo: {vaga['titulo']}")
        print(f"Empresa: {vaga['empresa']}")
        print(f"Local: {vaga['localizacao']}")